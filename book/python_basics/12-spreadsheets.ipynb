{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90adcfc",
   "metadata": {},
   "source": [
    "# Data Journalism Lesson 12: Working with spreadsheets\n",
    "\n",
    "The spreadsheet is dead. Long live the spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f69e68",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# Keep hold of the real method\n",
    "_orig_should_run = InteractiveShell.should_run_async\n",
    "\n",
    "# Wrap it so that any DeprecationWarning it emits is silenced\n",
    "def should_run_async(self, code, *args, **kwargs):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "        return _orig_should_run(self, code, *args, **kwargs)\n",
    "\n",
    "# Apply the monkeyâ€‘patch\n",
    "InteractiveShell.should_run_async = should_run_async\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03afc653",
   "metadata": {
    "tags": [
     "hide-input",
     "skip-execution",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import micropip\n",
    "await micropip.install('pyjanitor') \n",
    "await micropip.install('openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb63f4",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup code for the notebook\n",
    "import pandas as pd\n",
    "import janitor  # For clean_names()\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "# --- Helper functions for grading exercises ---\n",
    "\n",
    "def check_load_libraries(result):\n",
    "    # Check if pandas and janitor are imported\n",
    "    if 'pd' in globals() and 'janitor' in globals():\n",
    "        display(HTML('<div style=\"background-color: #dff0d8; padding: 10px; border-radius: 5px;\"><strong>Correct!</strong> Pandas and Janitor are imported.</div>'))\n",
    "    else:\n",
    "        missing = []\n",
    "        if 'pd' not in globals(): missing.append('pandas')\n",
    "        if 'janitor' not in globals(): missing.append('janitor')\n",
    "        display(HTML(f'<div style=\"background-color: #f2dede; padding: 10px; border-radius: 5px;\"><strong>Not quite!</strong> Make sure you import the following libraries: {\", \".join(missing)}.</div>'))\n",
    "\n",
    "def check_load_data_exercise(df):\n",
    "    # Check if df is a DataFrame and has specific columns from the initial read (before skip)\n",
    "    if isinstance(df, pd.DataFrame) and '...2' in df.columns and '...3' in df.columns:\n",
    "         display(HTML(f'<div style=\"background-color: #dff0d8; padding: 10px; border-radius: 5px;\"><strong>Great work!</strong> You imported the spreadsheet. Notice the messy headers initially.</div>'))\n",
    "    elif isinstance(df, pd.DataFrame):\n",
    "         display(HTML(f'<div style=\"background-color: #f2dede; padding: 10px; border-radius: 5px;\"><strong>Almost!</strong> Check the file path and ensure you loaded the data into a DataFrame named emitters.</div>'))\n",
    "    else:\n",
    "        display(HTML('<div style=\"background-color: #f2dede; padding: 10px; border-radius: 5px;\"><strong>Not quite!</strong> Make sure youve loaded the data into a pandas DataFrame.</div>'))\n",
    "\n",
    "def check_head_display(result):\n",
    "    # Simple check, assumes user ran .head()\n",
    "    display(HTML('<div style=\"background-color: #dff0d8; padding: 10px; border-radius: 5px;\"><strong>Code Ran.</strong> Make sure you see the first few rows of the DataFrame.</div>'))\n",
    "\n",
    "def check_load_data2_exercise(df, expected_header_part):\n",
    "    # Check if df is a DataFrame and the header looks correct after skipping\n",
    "    if isinstance(df, pd.DataFrame) and expected_header_part in df.columns:\n",
    "         display(HTML(f'<div style=\"background-color: #dff0d8; padding: 10px; border-radius: 5px;\"><strong>Correct!</strong> You skipped the first 3 rows successfully. The headers look much better.</div>'))\n",
    "    elif isinstance(df, pd.DataFrame):\n",
    "         display(HTML(f'<div style=\"background-color: #f2dede; padding: 10px; border-radius: 5px;\"><strong>Almost!</strong> Did you specify `skiprows=3` correctly? Check the resulting column names.</div>'))\n",
    "    else:\n",
    "        display(HTML('<div style=\"background-color: #f2dede; padding: 10px; border-radius: 5px;\"><strong>Not quite!</strong> Make sure your result is a pandas DataFrame.</div>'))\n",
    "\n",
    "def check_load_data3_exercise(df):\n",
    "    # In pandas, guess_max isn't a direct param. We check skip again.\n",
    "     if isinstance(df, pd.DataFrame) and 'Facility Name' in df.columns:\n",
    "         display(HTML(f'<div style=\"background-color: #dff0d8; padding: 10px; border-radius: 5px;\"><strong>Code Ran.</strong> You correctly used `skiprows=3`. Pandas handles type guessing automatically.</div>'))\n",
    "     else:\n",
    "         display(HTML('<div style=\"background-color: #f2dede; padding: 10px; border-radius: 5px;\"><strong>Not quite!</strong> Ensure you are still using `skiprows=3`.</div>'))\n",
    "\n",
    "def check_load_data4_exercise(df):\n",
    "    # Check if df is a DataFrame and names are cleaned\n",
    "    if isinstance(df, pd.DataFrame) and 'facility_name' in df.columns and 'total_reported_direct_emissions' in df.columns:\n",
    "         display(HTML(f'<div style=\"background-color: #dff0d8; padding: 10px; border-radius: 5px;\"><strong>Excellent!</strong> You skipped rows and cleaned the column names using `janitor.clean_names()`.</div>'))\n",
    "    elif isinstance(df, pd.DataFrame) and 'Facility Name' in df.columns:\n",
    "         display(HTML(f'<div style=\"background-color: #f2dede; padding: 10px; border-radius: 5px;\"><strong>Almost!</strong> Did you pipe the result of `read_excel` to `janitor.clean_names()`?</div>'))\n",
    "    else:\n",
    "        display(HTML('<div style=\"background-color: #f2dede; padding: 10px; border-radius: 5px;\"><strong>Not quite!</strong> Make sure you are reading the file with `skiprows=3` and then cleaning the names.</div>'))\n",
    "\n",
    "# --- Load Data & Define Variables ---\n",
    "# Define state (e.g., Minnesota 'MN' like previous notebook, or 'NE' like Rmd default)\n",
    "state_abbr = \"MN\" # Or choose another state like \"NE\"\n",
    "state_name = \"Minnesota\"\n",
    "\n",
    "# Path to the states lookup file (assuming it's in _static)\n",
    "excel_file_path = \"../_static/greenhouse-gas-emitters/ghgp_data_2022.xlsx\" # Path relative to notebook\n",
    "\n",
    "# Load and clean data for use in text\n",
    "try:\n",
    "    clean_emitters_df = pd.read_excel(excel_file_path, skiprows=3).clean_names()\n",
    "    state_emitters_count = clean_emitters_df[clean_emitters_df['state'] == state_abbr].shape[0]\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Emitters Excel file not found at {excel_file_path}\")\n",
    "    print(\"Please ensure the file exists and the path is correct.\")\n",
    "    clean_emitters_df = pd.DataFrame() # Create empty df to avoid errors later\n",
    "    state_emitters_count = 0\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading emitters data: {e}\")\n",
    "    clean_emitters_df = pd.DataFrame()\n",
    "    state_emitters_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968440d5",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init",
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from pyodide.http import pyfetch\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# Fetch the file asynchronously\n",
    "response = await pyfetch(url=\"../_static/greenhouse-gas-emitters/ghgp_data_2022.xlsx\", method=\"GET\")\n",
    "data_bytes = await response.bytes()\n",
    "excel_data = BytesIO(data_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0d979",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from myst_nb import glue\n",
    "\n",
    "# Glue variables for use in markdown cells\n",
    "glue(\"state_name\", state_name, display=False)\n",
    "glue(\"state_abbr\", state_abbr, display=False)\n",
    "glue(\"state_emitters_count\", f\"{state_emitters_count:,}\", display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59457d",
   "metadata": {},
   "source": [
    "## The Goal\n",
    "\n",
    "In this lesson, you'll learn how to pull spreadsheets into dataframes, a common challenge in data journalism. The spreadsheet is still by far the most common data analysis tool, in spite of all of its limitations. By the end of this tutorial, you'll understand two methods for converting spreadsheets into dataframes using the most common spreadsheet platforms: Excel and Google Sheets. You'll also practice cleaning and organizing the extracted data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a7bad8",
   "metadata": {},
   "source": [
    "## What is Data Journalism?\n",
    "\n",
    "Ask just about any data journalist what tool they started with and you'll almost universally get the same answer: The spreadsheet. For the old timers, spreadsheets were like magic back when computers were relatively new and the idea of using data to do journalism was a form of punk rock -- rebellious almost.\n",
    "\n",
    "Coulter Jones, now a data reporter at Bloomberg News, started his data journalism career in college, where he learned spreadsheets in a class.\n",
    "\n",
    "\"When you're in college or you're around people who are doing this, it's easy to compare yourself to people more advanced and feeling like, I don't know what I'm doing,\" he said. \"And then you would work with colleagues who you literally see them counting paper or going through and doing like a control F or something, you know, and it's like, what are you doing?\n",
    "\n",
    "\"Then you sort of realize that like almost every story is about the most or the least. I mean, it's that basic. It's like, who did the most? Where are the most fines going? Who raised the most money this campaign cycle?\"\n",
    "\n",
    "It sounds simple, but those kinds of basic questions drove a project Jones worked on that won a Pulitzer Prize in 2023 about federal officials trading stocks in companies they regulated. The first questions editors had? Who owns certain stocks? (A simple filter). Who has the most money? (A group by and summarize).\n",
    "\n",
    "Spreadsheets are everywhere in business and government. They're relatively easy to use -- you can learn 80-90 percent of what you need to learn in a day -- and extremely flexible for something organized into rows and columns. As spreadsheets have evolved, developers have added ways to add color and type and even charts made with few clicks.\n",
    "\n",
    "That has given a lot of spreadsheet users delusions of grandeur, who now want to create pretty spreadsheets instead of useful spreadsheets.\n",
    "\n",
    "It's likely in your time as a data journalist you'll have to use spreadsheets. If you've ever been to a NICAR conference, you'll note there are dozens of spreadsheet classes. The reason this textbook doesn't involve spreadsheets, outside of this lesson, is because they're out of step with modern data science and modern journalism.\n",
    "\n",
    "Modern data science/data journalism requires transparency. What does that mean? Notebooks, with code and data, are publicly available so anyone can look at what you did and decide for themselves if you did a good job or not.\n",
    "\n",
    "Can you do that with a spreadsheet? My argument is no. There's no way to audit your work, there's no way to reliably show how you did what you did step by step. A good faith argument is that it can be done, it's just very hard. If you miss a step, your analysis will appear wrong to someone coming behind.\n",
    "\n",
    "\"There's a difference between doing a quick daily story where you're just doing a pivot table or something and you write it and it's done ... versus like a longer term project or investigation and you go through that fact check and someone says, well, why are we using this number and not this other one?\" Jones told me.\n",
    "\n",
    "So how can you apply modern data science/data journalism principles in a world still awash in spreadsheets? You can pull that spreadsheet into Python using pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f88f2",
   "metadata": {},
   "source": [
    "## The Basics\n",
    "\n",
    "How do you pull a spreadsheet into Python? Well, depending on the sheet, it might be *very* easy. But if the data provider you are working with tried to make it pretty, it can get a little worse.\n",
    "\n",
    "One near-universal truth you should keep in mind as you get better at programming: If you have a problem, someone else probably had it first and they very likely made a library that solves that problem. Do you think there are developers in the world who wanted to pull a spreadsheet into Python? Indeed there has been.\n",
    "\n",
    "The library we're going to work with today is `pandas`, specifically its `read_excel` function. You don't have to be a genius to figure out what this function does. We're going to load all of our libraries first and then contend with the data. One other library we're going to need -- `pyjanitor` (imported as `janitor`). Think of `janitor` as the library that's going to have to clean up all this spreadsheet glitter someone thought would make it sparkle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980af4c5",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import janitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f4d67",
   "metadata": {},
   "source": [
    "The dataset we're going to work with is from a U.S. Environmental Protection Agency program called the [Greenhouse Gas Reporting Program](https://www.epa.gov/ghgreporting/data-sets). The program collects greenhouse gas emissions from \"large emitting facilities, suppliers of fossil fuels and industrial gases that result in GHG emissions when used, and facilities that inject carbon dioxide underground.\" These facilities, all over the country, are interesting data to look at when reporting on climate change at a local and state level and ... come in an Excel spreadsheet.\n",
    "\n",
    "If you were to download the file and open it, you'd see an immediate problem: The first row of the spreadsheet is not the header row. In every dataset we've used so far, the first row is the header row, because that's what `read_csv` expects. It knows that most datasets have a header row, and it's first, so it takes that row and makes it the header. The second row in the csv is the first row of data. Here's what your spreadsheet looks like:\n",
    "\n",
    "```{image} ../figures/spreadsheet1.png\n",
    ":alt: Screenshot of Excel spreadsheet showing header info in first few rows\n",
    ":width: 100%\n",
    ":align: center\n",
    "```\n",
    "\n",
    "See how the first three rows are notes? What do we do about that?\n",
    "\n",
    "First, we're going to use a function in `pandas` that should look and act *very* familiar: It's `pd.read_excel`. It does what you think it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36863a",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "emitters = pd.read_excel(excel_data)\n",
    "emitters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0065f",
   "metadata": {},
   "source": [
    "That ... doesn't look familiar at all.\n",
    "\n",
    "What does `emitters` look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9d0950",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "emitters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c51b20",
   "metadata": {},
   "source": [
    "It's a mess. First of all, our headers are all named `...2`, `...3`, `...4` and so on (pandas generates default names when the header row is problematic or missing). You can see what looks more like a header row in row 3 (index 2), which would actually be row 4 of the Excel sheet since `read_excel` tried to use row 1 (index 0) as a header row.\n",
    "\n",
    "All along, we've been using `read_csv` with the defaults -- first row is the header, guess what each column is based on the first few hundred rows, no special processing instructions. This is what it looks like when those things aren't the case.\n",
    "\n",
    "What we need to be able to tell `read_excel` is that it should just ignore those first three rows and use the 4th row (index 3 in 0-based indexing) as the header.\n",
    "\n",
    "Handy enough: `read_excel` (and `read_csv`) both have a `skiprows` parameter. You can tell it to skip a certain number of rows *before* the header row. Our header row is in row 4 (Excel row number), so we need to skip the 3 rows above it.\n",
    "\n",
    "### Exercise 1: Skipping\n",
    "\n",
    "Use the `skiprows` parameter in `pd.read_excel` to skip the first 3 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ed523",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "emitters = pd.read_excel(excel_data, skiprows=____)\n",
    "\n",
    "print(emitters.head())\n",
    "\n",
    "check_load_data2_exercise(emitters, 'Facility Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d010add8",
   "metadata": {},
   "source": [
    "Now we're getting something else we haven't seen -- import errors. *[Note: In pandas, type inference issues often manifest as columns with `object` dtype containing mixed types, or sometimes errors during later operations, rather than explicit warnings.]* What that *could* mean is based on the first N rows pandas samples, it's expecting one type of data, but it's getting a different kind later on. That might mean the first rows were numbers, but now there's text in the column and that's creating problems. It might mean the first rows had good dates and now they've gone bad. The point is, the guesses can sometimes be wrong.\n",
    "\n",
    "Sometimes this is a major problem. Often, pandas handles mixed types by assigning the `object` dtype, which can be less efficient but generally works. If specific type issues arise, you might need to specify `dtype` options in `read_excel`.\n",
    "\n",
    "For now, let's assume pandas' default guessing is sufficient and move on. We successfully skipped the rows.\n",
    "\n",
    "### Guessing\n",
    "\n",
    "If you encountered type errors (which we didn't explicitly see here, but could happen with other messy sheets), you might need to:\n",
    "1.  Read the file without type inference (`dtype=object`) and convert types later.\n",
    "2.  Specify the correct `dtype` for problematic columns directly in `read_excel`.\n",
    "3.  Load the data and then use functions like `pd.to_numeric`, `pd.to_datetime` with `errors='coerce'` to fix types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8befaa",
   "metadata": {},
   "source": [
    "One last thing we can do that we haven't done before -- there is nothing stopping you from chaining on other commands to your read step. In fact, you could do everything we've done so far in one single block, going from `pd.read_excel(...)` -> `.query(...)` -> `.groupby(...)` -> `.agg(...)` and get output. It makes sense to separate your steps so you can diagnose problems where they are happening, but it's all very feasible to do everything in one giant code blob.\n",
    "\n",
    "Sometimes, adding a little more to your read step will make your life a lot easier later. You probably haven't noticed because we haven't really looked, but some of the column names in this data are really gnarly from the standpoint of Python's typical variable naming conventions (and pandas best practices). For example, one column is called \"Total reported direct emissions\" which has spaces in it that are awkward to work with (requiring `df['Total reported direct emissions']` syntax). Another is \"CO2 emissions (non-biogenic)\", which adds parens to our column name.\n",
    "\n",
    "Good thing you learned about `janitor.clean_names()` in Chapter 9. That would come in really handy here.\n",
    "\n",
    "### Exercise 3: Cleaning\n",
    "\n",
    "Chain the `janitor.clean_names()` function after reading the Excel file (with `skiprows`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c06785",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "emitters = (\n",
    "    pd.read_excel(excel_data, skiprows=____)\n",
    "    .____()\n",
    ")\n",
    "\n",
    "print(emitters.head())\n",
    "\n",
    "check_load_data4_exercise(emitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc0379",
   "metadata": {},
   "source": [
    "Looks a lot like what we're used to, yes? You've now got a dataset you can run queries on, like asking how many of these are in {glue:text}`state_name`. There are {glue:text}`state_emitters_count`, if you're curious.\n",
    "\n",
    "One thing to know: Spreadsheets *can* contain more than one sheet in them. This one actually contains 11 of them -- most of them subsets of the data, but some are completely different formats and structures. `pd.read_excel` by default just grabs sheet 1 (using a 0-based index, so `sheet_name=0`), unless you tell it otherwise. To do so, you'd just add `sheet_name=1` inside `read_excel` to get sheet 2, `sheet_name='SheetName'` to get a specific sheet by name, or `sheet_name=None` to get a dictionary of DataFrames, one for each sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b8d65a",
   "metadata": {},
   "source": [
    "## Pulling data from Google Sheets\n",
    "\n",
    "Part of the reason why Excel spreadsheets are everywhere is because it's the spreadsheet program that's stood the test of time. Microsoft first launched it in 1985 to compete with another product that no longer exists. Excel won.\n",
    "\n",
    "But in the internet era, another spreadsheet platform is giving Excel a run for its money. Most current college students won't remember a time without Google Sheets, launched in 2006, and for most of them, because of Chromebooks in high school, Sheets was the first spreadsheet platform they'd ever seen.\n",
    "\n",
    "That it comes in at the low, low price of free means a growing number of providers are using it to release data, particularly at the state and local levels.\n",
    "\n",
    "Because of limitations in the tutorial data and how Google handles authorizations, I can't directly demonstrate it here, but there are several Python libraries like `gspread` and `pandas` itself (using specific URLs) that handle reading Google Sheets into a dataframe. The concepts are similar, but authentication is usually the trickiest part, often requiring API keys or OAuth2.\n",
    "\n",
    "If you have a **publicly published** Google Sheet (specifically, published to the web as a CSV), you can often read it directly with `pd.read_csv` using a specially formatted URL.\n",
    "\n",
    "For sheets requiring authentication, `gspread` is a popular choice:\n",
    "\n",
    "```python\n",
    "# Example using gspread (requires setup and authentication)\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "import pandas as pd\n",
    "import janitor\n",
    "\n",
    "# --- Authentication Setup (replace with your actual credentials) ---\n",
    "scope = ['https://www.googleapis.com/auth/spreadsheets.readonly']\n",
    "creds_file = 'path/to/your/credentials.json' # You need to get this from Google Cloud\n",
    "creds = Credentials.from_service_account_file(creds_file, scopes=scope)\n",
    "client = gspread.authorize(creds)\n",
    "# --- End Authentication Setup ---\n",
    "\n",
    "# Open the Google Sheet by URL or title\n",
    "sheet_url = 'YOUR_GOOGLE_SHEET_URL_HERE'\n",
    "spreadsheet = client.open_by_url(sheet_url)\n",
    "\n",
    "# Select a specific worksheet (e.g., the first one)\n",
    "worksheet = spreadsheet.get_worksheet(0) # 0-based index\n",
    "\n",
    "# Get all values as a list of lists and convert to DataFrame\n",
    "data = worksheet.get_all_values()\n",
    "headers = data.pop(0) # Assumes first row is header\n",
    "gsheet_df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "# Now you can apply skiprows-like logic (if needed) and clean names\n",
    "# Example: If header was actually row 4 (index 3)\n",
    "# data = worksheet.get_all_values()\n",
    "# headers = data[3] # Get header from 4th row\n",
    "# actual_data = data[4:] # Get data from 5th row onwards\n",
    "# gsheet_df = pd.DataFrame(actual_data, columns=headers)\n",
    "# gsheet_df = gsheet_df.clean_names()\n",
    "\n",
    "print(gsheet_df.head())\n",
    "```\n",
    "\n",
    "The key takeaway is that while the tool changes, the core data manipulation concepts (handling headers, cleaning names) remain the same once the data is in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5001bf99",
   "metadata": {},
   "source": [
    "## The Recap\n",
    "\n",
    "Throughout this lesson, you've mastered essential techniques for working with spreadsheet data in Python using pandas. You've learned to use `pd.read_excel()` to import Excel files, handling challenges like skipping rows using `skiprows`. You've also explored how to clean column names using the `pyjanitor` package's `clean_names()` function and discussed how to approach accessing data from Google Sheets using libraries like `gspread`. Remember, while spreadsheets are ubiquitous in data journalism, importing them into pandas allows for more transparent, reproducible analysis within your Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66314ecd",
   "metadata": {},
   "source": [
    "## Terms to Know\n",
    "\n",
    "- **`pandas`**: A fundamental Python library for data manipulation and analysis.\n",
    "- **`pd.read_excel()`**: A function in the `pandas` library used to read Excel files (.xls, .xlsx) into a DataFrame.\n",
    "- **`skiprows`**: A parameter in `pd.read_excel()` (and other pandas read functions) that allows you to ignore a specified number of rows at the beginning of a spreadsheet before the header.\n",
    "- **`sheet_name`**: A parameter in `pd.read_excel()` to specify which sheet(s) to read from an Excel file (by 0-based index, name, or `None` for all).\n",
    "- **`pyjanitor`**: A Python library providing convenient functions for cleaning data, including `clean_names()`.\n",
    "- **`janitor.clean_names()`**: A function from the `pyjanitor` package that standardizes column names (e.g., converts to snake_case) for easier data manipulation.\n",
    "- **`gspread`**: A Python library that enables reading and writing Google Sheets directly from Python (often requires authentication setup).\n",
    "- **Excel**: A spreadsheet program developed by Microsoft, widely used for data storage and analysis.\n",
    "- **Google Sheets**: A web-based spreadsheet program offered by Google, part of the Google Workspace suite."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
