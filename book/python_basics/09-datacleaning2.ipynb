{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0776b0d",
   "metadata": {},
   "source": [
    "# Data Journalism Lesson 9: Data Cleaning with pyjanitor\n",
    "\n",
    "Clean up your data with code using the `pyjanitor` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d9da7",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# Keep hold of the real method\n",
    "_orig_should_run = InteractiveShell.should_run_async\n",
    "\n",
    "# Wrap it so that any DeprecationWarning it emits is silenced\n",
    "def should_run_async(self, code, *args, **kwargs):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "        return _orig_should_run(self, code, *args, **kwargs)\n",
    "\n",
    "# Apply the monkeyâ€‘patch\n",
    "InteractiveShell.should_run_async = should_run_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df23aa8d",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init",
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import micropip\n",
    "await micropip.install('pyjanitor') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3f33de",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup code for the notebook\n",
    "import pandas as pd\n",
    "import janitor  # Import pyjanitor\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Helper functions for grading exercises\n",
    "def display_feedback(correct, message_correct, message_incorrect):\n",
    "    if correct:\n",
    "        display(HTML(f'<div style=\"background-color: #dff0d8; padding: 10px; border-radius: 5px;\"><strong>Correct!</strong> {message_correct}</div>'))\n",
    "    else:\n",
    "        display(HTML(f'<div style=\"background-color: #f2dede; padding: 10px; border-radius: 5px;\"><strong>Not quite!</strong> {message_incorrect}</div>'))\n",
    "\n",
    "def check_read_csv(df, expected_rows):\n",
    "    correct = isinstance(df, pd.DataFrame) and len(df) == expected_rows\n",
    "    msg_correct = 'Great work! You imported your state\\'s census of governments.'\n",
    "    msg_incorrect = 'Check the file path or the read_csv function. Did you replace ____ with your state?'\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        msg_incorrect = 'The variable `governments` should be a pandas DataFrame.'\n",
    "    elif len(df) != expected_rows:\n",
    "         msg_incorrect = f'Expected {expected_rows} rows, but got {len(df)}. Did you load the correct state\\'s data?'\n",
    "    display_feedback(correct, msg_correct, msg_incorrect)\n",
    "\n",
    "def check_glimpse(df_info_output):\n",
    "    # info() prints to stdout and returns None.\n",
    "    # We assume if the code runs and info_output is None, the user called it.\n",
    "    correct = df_info_output is None\n",
    "    msg_correct = 'Good job using .info() to inspect the DataFrame!'\n",
    "    msg_incorrect = 'Make sure you are calling the .info() method on the `governments` DataFrame.'\n",
    "    display_feedback(correct, msg_correct, msg_incorrect)\n",
    "\n",
    "def check_clean_names(df):\n",
    "    correct = False\n",
    "    msg_correct = 'Excellent! The column names are now cleaned (snake_case).'\n",
    "    msg_incorrect = 'Make sure you applied the .clean_names() method correctly.'\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        # Check if names look like snake_case (simple check)\n",
    "        are_snake_case = all([' ' not in col and col.lower() == col for col in df.columns])\n",
    "        if are_snake_case:\n",
    "             correct = True\n",
    "        else:\n",
    "             msg_incorrect = 'The column names do not appear to be in snake_case. Did you use .clean_names()?'\n",
    "    else:\n",
    "        msg_incorrect = 'Expected a DataFrame as output.'\n",
    "    display_feedback(correct, msg_correct, msg_incorrect)\n",
    "\n",
    "def check_remove_empty(df, original_df):\n",
    "    correct = isinstance(df, pd.DataFrame) # Basic check\n",
    "    msg_correct = 'Good! You\\'ve applied the remove_empty() method.'\n",
    "    msg_incorrect = 'Make sure you are calling .remove_empty() on the cleaned DataFrame.'\n",
    "    # More robust check could compare shapes if empty rows/cols were expected\n",
    "    display_feedback(correct, msg_correct, msg_incorrect)\n",
    "\n",
    "def check_tabyl(result_df, expected_col):\n",
    "    correct = False\n",
    "    msg_correct = f'Successfully generated the frequency table for {expected_col}!'\n",
    "    msg_incorrect = f'Could not verify the frequency table for {expected_col}.'\n",
    "    if isinstance(result_df, pd.DataFrame):\n",
    "        # janitor.tabyl column names can vary slightly based on version/usage\n",
    "        count_col_present = 'n' in result_df.columns or 'count' in result_df.columns\n",
    "        percent_col_present = 'percent' in result_df.columns or 'percentage' in result_df.columns\n",
    "        if expected_col in result_df.columns and count_col_present and percent_col_present:\n",
    "            correct = True\n",
    "        else:\n",
    "            msg_incorrect = f'Expected columns like \\'{expected_col}\\', a count column (e.g., \\'n\\', \\'count\\'), and a percent column (e.g., \\'percent\\') in the output DataFrame. Did you use janitor.tabyl correctly?'\n",
    "    else:\n",
    "        msg_incorrect = 'Expected a DataFrame output from janitor.tabyl().'\n",
    "    display_feedback(correct, msg_correct, msg_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21182179",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Load Data Setup ---\n",
    "state_full_name = \"Minnesota\"\n",
    "state_lower_url = state_full_name.lower().replace(' ', '-') # Handle spaces for URL\n",
    "dataurl = f'../_static/government-census/{state_lower_url}.csv'\n",
    "\n",
    "# Load the main dataset\n",
    "governments = pd.read_csv(dataurl)\n",
    "nrows = len(governments)\n",
    "ncolumns = len(governments.columns)\n",
    "\n",
    "countyLanguage = ('county', 'counties')\n",
    "\n",
    "# Pre-clean data for later exercises\n",
    "clean_governments = governments.clean_names().remove_empty() # Default removes both rows and cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84534fcd",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Glue variables for use in markdown\n",
    "from myst_nb import glue\n",
    "\n",
    "glue(\"state_full_name\", state_full_name, display=False)\n",
    "glue(\"nrows\", nrows, display=False)\n",
    "glue(\"ncolumns\", ncolumns, display=False)\n",
    "glue(\"county_singular\", countyLanguage[0], display=False)\n",
    "glue(\"county_plural\", countyLanguage[1], display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492accc7",
   "metadata": {},
   "source": [
    "## The Goal\n",
    "\n",
    "In this lesson, you'll learn how to use the `pyjanitor` package to clean and standardize your data more efficiently. By the end of this tutorial, you'll understand how to clean column names, remove empty rows and columns, identify duplicates, and explore data consistency using `pyjanitor` functions. These skills will help you prepare messy datasets for analysis more quickly and reliably, saving you time and reducing errors in your data journalism work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5617a",
   "metadata": {},
   "source": [
    "## What is Data Journalism?\n",
    "\n",
    "`pyjanitor` is one of those Python packages that honestly, I never knew I needed until I *really* needed it. Now it's one of the packages I use the most.\n",
    "\n",
    "What does `pyjanitor` do? Think of it like a Swiss Army Knife of data cleaning for pandas DataFrames. It's got a knife, some scissors, a file, a bottle opener, and on and on. Do they all go together? It doesn't seem like it until you're camping and having all of those things in a small package is really handy.\n",
    "\n",
    "Enter `pyjanitor`. Someday, you'll get a dataset with 300 *columns* of data and most of them have names that don't conform to Python's conventions (or just good practice). Names like \"1st Quarter 2024 Actual\" or \"2024 Estimated Revenue\". Spaces, numbers starting columns, symbols -- a horrorshow. And `pandas` will dutifully read that, but using those column names in your code is a pain (requiring `df['Column Name With Spaces']` syntax).\n",
    "\n",
    "What if you could fix them all at once? You can with `pyjanitor`. It has a `clean_names()` method that does this.\n",
    "\n",
    "Did you get some funky spreadsheet from a government entity with lots of blank rows and columns? `pyjanitor` can remove columns and rows with no data in them automatically using `remove_empty()`.\n",
    "\n",
    "Using Python to try and read in hundreds of spreadsheets programmatically, but those spreadsheets put the header row on line 6 instead of line 1 like you're supposed to? `pyjanitor` can handle that with `skip_header()` (though `pandas.read_csv` also has a `header` argument).\n",
    "\n",
    "And a lot more. Let's explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8bd642",
   "metadata": {},
   "source": [
    "## The Basics\n",
    "\n",
    "The bane of every data analyst's existence is data cleaning.\n",
    "\n",
    "Every developer, every data system, every agency, they all have opinions about how data gets collected. Some decisions make sense from the outside. Some decisions are based entirely on internal politics: who is creating the data, how they are creating it, why they are creating it. Is it automated? Is it manual? Are data normalized -- meaning all the spellings are the same? Are there free form fields where users can just type into or does the system restrict them to choices?\n",
    "\n",
    "Your question -- what you want to do with the data -- is almost never part of that equation.\n",
    "\n",
    "So cleaning data is the process of fixing issues in your data so you can answer the questions you want to answer. Unfortunately, there's no template here. There's no checklist. It's just a big bag of tricks that eventually runs out and you'll be left fixing individual issues by hand, if it's really bad.\n",
    "\n",
    "But let's start simple. There are certain tricks that we can start with to make our lives easier. We'll slowly make it harder as we dig deeper.\n",
    "\n",
    "One of the first places we can clean data is cleaning the headers. Every system has their own way of recording headers, and every developer has their own thoughts of what a good idea is within it. Python (and pandas) is most happy when headers are one word (using underscores instead of spaces), lower case, without special characters (except underscores). This is often called \"snake_case\". Without telling you, I've been fixing these headers behind the scenes before you get the data. No more. Today, you're going to notice that pandas will read columns that have spaces or start with numbers or have special characters in them, but accessing them requires bracket notation (e.g., `df['Column Name']`) instead of dot notation (e.g., `df.column_name`).\n",
    "\n",
    "There is an external library in Python called `pyjanitor` that makes fixing headers trivially simple.\n",
    "\n",
    "First things first, as always: we load the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3014083b",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import janitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dea1b8",
   "metadata": {},
   "source": [
    "Now we load our data. I'm writing this chapter during the last few months of the 2024 Presidential Election. A common task for journalists during elections? Fact-checking. A common trope in elections are claims that government has become too big, too bloated, too expensive. Your first thought, as a reporter, should be simply: Has it? A way to check? The U.S. Census Bureau. The same agency who counts people and estimate populations between counts do a ton of other enumerations in and around American society. One is the annual survey of governments, where the Census Bureau asks every government -- state, {glue:text}`county_singular`, city, school district -- how many people they employ and what their payroll is. It's a fantastic dataset for comparing your community to others like it, in your state and in the nation. Got a mayoral candidate claiming the city has tripled the number of people working there and is out of control? You can check that.\n",
    "\n",
    "We get this data the same way we have all along:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1bc15b",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "governments = pd.read_csv(\"../_static/government-census/minnesota.csv\")\n",
    "governments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d3571",
   "metadata": {},
   "source": [
    "## Cleaning headers\n",
    "\n",
    "You can get a sense of the problems with the headers in this data with a quick `.info()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca817a",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "governments._____()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdfd0a2",
   "metadata": {},
   "source": [
    "Look at the column names listed. You'll likely see names with spaces and mixed capitalization like `Name of Government` or `Type of Government`. If you wanted to use this column in a filter, it would look like this: `governments[governments['Type of Government'] == 'State']`. That's a lot of extra things to go wrong -- did you capitalize correctly, did you remember the quotes around the column name? It would be better if we could simplify this and reduce the number of places where we could go wrong by converting to snake_case (`type_of_government`).\n",
    "\n",
    "`pyjanitor` makes this easy to fix. How easy? This easy. `pyjanitor` provides a `.clean_names()` method that you can chain onto your DataFrame.\n",
    "\n",
    "### Exercise 1: Cleaning up column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d21e74b",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "cleaned_df_names = governments._____()\n",
    "cleaned_df_names.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3662afdc",
   "metadata": {},
   "source": [
    "Just like that, all lowercase, all one word (using underscores), no awkward bracket notation necessary to confuse our code later on.\n",
    "\n",
    "### Exercise 2: Dropping empty columns and rows\n",
    "\n",
    "Another good trick by `pyjanitor` is easily dropping empty columns and rows. Sometimes columns are in the data and there's nothing in them. Nada. Blank. Rarer, but still possible: Rows of blank data. We could use pandas' `.dropna()` but `pyjanitor`'s `remove_empty()` provides a convenient wrapper specifically for completely empty rows/columns.\n",
    "\n",
    "Because this is data from the Census Bureau, an agency who exists to create data, we have a minimum of weirdness to deal with. But a good plan when you begin working with data is to inspect the data after you import it, and then apply `pyjanitor` functions to clean it up. `pyjanitor` has a function called `.remove_empty()` which by default removes both empty rows and columns. You can specify `axis='rows'` or `axis='columns'` if needed.\n",
    "\n",
    "This likely won't remove anything from this specific dataset, but this is how you would do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67417506",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "cleaned_and_emptied_df = governments.clean_names()._____()\n",
    "cleaned_and_emptied_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4753309",
   "metadata": {},
   "source": [
    "Let's save what we've done so we can use it for the rest of the exercise into a new dataframe called `clean_governments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6bb22",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Save the cleaned data (re-assigning based on previous step)\n",
    "clean_governments = cleaned_and_emptied_df\n",
    "\n",
    "# Verify by checking columns\n",
    "print(clean_governments.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e97ee",
   "metadata": {},
   "source": [
    "## Inconsistency\n",
    "\n",
    "`pandas` itself also has some handy tools for our data smells. One is called `value_counts`, which creates a frequency table of unique records in a single field. All you need to do is call `dataframe.value_counts('column_name')` with the DataFrame and column name.\n",
    "\n",
    "### Exercise 3: Data smells with `value_counts`\n",
    "\n",
    "So does the Census Bureau record the names of governments consistently? `value_counts` will tell us and will tell us a little bit about the data. Remember: we've cleaned the headers, so it's now `name_of_government`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d55f0a",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "name_counts = clean_governments._____(_____)\n",
    "name_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643c538",
   "metadata": {},
   "source": [
    "What you are looking for here are mistakes. For a made up example: Let's pretend you had a town named Adams. Do you see `adams` and then `ADAMS` (after cleaning)? There's a chance that's a mistake (though `clean_names` helps with case). More likely, you might see variations like `adams county` and `adams city`. There's a chance you might have two different levels of government called that. Data smells are just a warning -- they aren't a guarantee of problems.\n",
    "\n",
    "### Exercise 4: Combining what we know\n",
    "\n",
    "What if you had a legislative candidate complaining about how expensive running the jails had become? We can add a pandas `filter` (using boolean indexing) before we run `value_counts` to give specific data the smell test. To find jails (and prisons), we want to filter the `government_function` column for 'Corrections'. Then, we can run `value_counts` on the `name_of_government` column for that filtered data to see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1ea18",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "corrections_df = clean_governments[clean_governments['government_function'] == _____]\n",
    "corrections_counts = corrections_df._____(_____)\n",
    "print(corrections_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2d399",
   "metadata": {},
   "source": [
    "Depending on how your state is set up, you might see one row for your state corrections organization -- probably under your state's name -- and one row for each {glue:text}`county_singular` jail. If you see counts with a number other than 1, that's an immediate question you'll need to answer before you start doing data analysis. Why are there multiple entries for the same government name within the 'Corrections' function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02747043",
   "metadata": {},
   "source": [
    "## The Recap\n",
    "\n",
    "Throughout this lesson, you've learned several key data cleaning techniques using the `pyjanitor` package with pandas. You've practiced cleaning column names with `.clean_names()`, removing empty rows and columns with `.remove_empty()`, and exploring data consistency with `value_counts`. Remember, data cleaning is an essential step in any data analysis project, and the tools you've learned here will help you tackle common data issues more efficiently. As you work with different datasets, you'll find these `pyjanitor` functions invaluable for quickly standardizing and exploring your data before diving into deeper analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99694777",
   "metadata": {},
   "source": [
    "## Terms to Know\n",
    "\n",
    "- **`pyjanitor`**: A Python package providing convenient data cleaning routines for pandas DataFrames.\n",
    "- **`.clean_names()`**: A `pyjanitor` method that cleans column names, typically converting them to snake_case.\n",
    "- **`.remove_empty()`**: A `pyjanitor` method that removes empty rows and/or columns from a DataFrame.\n",
    "- **Data cleaning**: The process of identifying and correcting errors or inconsistencies in datasets.\n",
    "- **Duplicates**: Multiple identical records in a dataset that may skew analysis results (can be found with pandas' `.duplicated()` and `.drop_duplicates()`).\n",
    "- **Data consistency**: The uniformity and reliability of data across a dataset.\n",
    "- **snake_case**: A naming convention where words are lowercase and separated by underscores (e.g., `column_name`)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
